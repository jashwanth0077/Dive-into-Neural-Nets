# Dive into Neural Networks  

**Course:** Artificial Intelligence and Machine Learning  

---

## ğŸ“Œ Overview  
This project was developed as part of the **Artificial Intelligence and Machine Learning** course.  
It focuses on understanding and implementing fundamental neural network architectures **from scratch**, without using high-level libraries like TensorFlow or PyTorch.  

---

## ğŸš€ Implemented Models  

### 1. ğŸ¦– Dino Name Generator (RNN)  
- Implemented a **Recurrent Neural Network (RNN)** to generate creative dinosaur names.  
- The model was trained on a dataset of dinosaur names to **learn sequential patterns** and generate new names resembling the style of training data.  
- Built completely from scratch, handling character-level embeddings, sequence modeling, and softmax-based prediction.  

### 2. âœï¸ Handwritten Digit Recognition (CNN)  
- Constructed and trained a **Convolutional Neural Network (CNN)** from scratch to classify handwritten digits (MNIST-like dataset).  
- Implemented essential CNN components manually:  
  - **Convolution layers**  
  - **ReLU activation**  
  - **MaxPooling**  
  - **Flattening**  
  - **Fully Connected (FC) layers**  
  - **Softmax classification**  
- Achieved accurate recognition performance by careful training and optimization.  

---

## ğŸ› ï¸ Key Learnings  
- Gained **hands-on experience** in designing and training neural networks without relying on deep learning frameworks.  
- Understood the **mathematical foundations** behind RNNs and CNNs.  
- Learned the challenges of gradient flow, overfitting, and optimization while building models from scratch.  

---

## ğŸ“‚ Repository Structure  

---

## ğŸ‘¨â€ğŸ« Acknowledgements  
- Guided by **Prof. Swaprava Nath**  
- Course: *Artificial Intelligence and Machine Learning (Spring 2023)*  

---

## ğŸ“Œ Future Work  
- Extend Dino Name Generator with **LSTM/GRU** for longer sequence memory.  
- Enhance CNN with **data augmentation** and **regularization techniques** for better generalization.  
